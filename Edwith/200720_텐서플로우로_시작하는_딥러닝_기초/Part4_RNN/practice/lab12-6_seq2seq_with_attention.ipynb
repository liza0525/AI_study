{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab12-6 Seq2Seq With Attention\n",
    "> Seq2Seq 성능을 높히기 위해 도입된 방법\n",
    "\n",
    "- 지난 시간 : Encoder-Decoder를 이용한 Seq2Seq 구현(translator 만들기)\n",
    "    - Encoder를 거쳐 나온 하나의 vector로는 많은 정보를 담기 어려움\n",
    "    - 문장이 길거나 많아지면 오히려 성능이 떨어지는 현상이 일어남\n",
    "    - 이를 해결하기 위해 Attention 기법을 이용\n",
    "    \n",
    "## Attention\n",
    "- 문장 번역 시 모든 형태소를 번역하는 것이 아닌 주요/핵심 단어만 번역하는 것\n",
    "- 모든 정보를 기억할 수 없으니, 특정 목적에 맞는 단어들만 저장\n",
    "### How does Attention implemet?\n",
    "- 이전에는 마지막 Hidden에 의해 만들어진 context vector를 기반으로 decoder 학습이 이루어짐\n",
    "- 하지만 attention에서는 매 step의 vector를 이용하여 context vector 도출\n",
    "    - attention weights를 이용하여 현재 step에서 가장 중요한 영향을 미칠 vector에 높은 가중치 부여\n",
    "    \n",
    "---\n",
    "## Reference\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "- [Neural Machine Translation with Attention from Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "### matplotlib 한글 폰트 설정 #############################\n",
    "from matplotlib import font_manager, rc\n",
    "## for window #####\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "## for mac #####\n",
    "#rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for source\n",
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder with Attention\n",
    "##### Score 도출\n",
    "- 현재 step에서 어떤 encoder가 중요한 지 반영하기 위해 각각의 score를 도출\n",
    "    - Luong's multiplicative style\n",
    "    - Bahdanau's additive style (이 강의에서는 이것을 이용)\n",
    "    \n",
    "##### Attention weights 이용\n",
    "- 위에서 구한 Score값을 이용하여 Attentions weights를 도출\n",
    "- 이 Attentions weights의 총합은 항상 1\n",
    "\n",
    "##### Context vector\n",
    "- 각 hidden 값에 구해준 weights를 곱한 후, 그 총합을 도출\n",
    "\n",
    "##### Attention vector\n",
    "- context vector와 hidden 값을 concat하여 Attention vector 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        \n",
    "        #### ★ 이 부분이 바다나우 계산법 이용한 것\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        \n",
    "        #### ★ Attention Weights\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        \n",
    "        #### ★ Context Vector\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './checkpoints/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0396 Batch Loss 0.9900\n",
      "Epoch 10 Loss 0.0378 Batch Loss 0.9451\n",
      "Epoch 20 Loss 0.0343 Batch Loss 0.8579\n",
      "Epoch 30 Loss 0.0326 Batch Loss 0.8143\n",
      "Epoch 40 Loss 0.0295 Batch Loss 0.7386\n",
      "Epoch 50 Loss 0.0219 Batch Loss 0.5466\n",
      "Epoch 60 Loss 0.0168 Batch Loss 0.4193\n",
      "Epoch 70 Loss 0.0121 Batch Loss 0.3022\n",
      "Epoch 80 Loss 0.0080 Batch Loss 0.2000\n",
      "Epoch 90 Loss 0.0049 Batch Loss 0.1227\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c3cc1c5c18>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restore checkpoint\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I feel hungry\n",
      "Predicted translation: 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
