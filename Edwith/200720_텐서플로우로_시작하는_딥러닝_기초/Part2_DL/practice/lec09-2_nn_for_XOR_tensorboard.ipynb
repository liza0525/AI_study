{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09 XOR - Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777) # for reproducibility\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary 값을 logs 폴더에 저장하고 아래 명령어를 실행하여 확인(http://127.0.0.1:6006)\n",
    "- tensorboard --logdir = ./logs/xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"./logs/xor\"\n",
    "writer = tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR using Deep NN\n",
    "- 4Layer NN을 학습시켜 모델 생성\n",
    "- Model의 값을 histogram으로 tensorboard에 저장\n",
    "- cost, accuracy의 값을 scalar 값으로 tensorboard에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal((2, 10)), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal((10,)), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal((10, 10)), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal((10,)), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal((10, 10)), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal((10,)), name='bias3')\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal((10, 1)), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
    "\n",
    "def neural_net(features, step):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "    \n",
    "    # tensorboard에 historam으로 저장\n",
    "    with writer.as_default():\n",
    "        tf.summary.histogram(\"weights1\", W1, step=step)\n",
    "        tf.summary.histogram(\"biases1\", b1, step=step)\n",
    "        tf.summary.histogram(\"layer1\", layer1, step=step)\n",
    "\n",
    "        tf.summary.histogram(\"weights2\", W2, step=step)\n",
    "        tf.summary.histogram(\"biases2\", b2, step=step)\n",
    "        tf.summary.histogram(\"layer2\", layer2, step=step)\n",
    "\n",
    "        tf.summary.histogram(\"weights3\", W3, step=step)\n",
    "        tf.summary.histogram(\"biases3\", b3, step=step)\n",
    "        tf.summary.histogram(\"layer3\", layer3, step=step)\n",
    "\n",
    "        tf.summary.histogram(\"weights4\", W4, step=step)\n",
    "        tf.summary.histogram(\"biases4\", b4, step=step)\n",
    "        tf.summary.histogram(\"hypothesis\", hypothesis, step=step)\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    \n",
    "    # tensorboard에 scalar로 저장\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('loss', cost, step=step)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    \n",
    "    # tensorboard(scalar) 저장\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('accuracy', accuracy, step=step)\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels, step):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features, step),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, W4, b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.9517\n",
      "Iter: 50, Loss: 0.6936\n",
      "Iter: 100, Loss: 0.6923\n",
      "Iter: 150, Loss: 0.6912\n",
      "Iter: 200, Loss: 0.6901\n",
      "Iter: 250, Loss: 0.6890\n",
      "Iter: 300, Loss: 0.6879\n",
      "Iter: 350, Loss: 0.6867\n",
      "Iter: 400, Loss: 0.6855\n",
      "Iter: 450, Loss: 0.6842\n",
      "Iter: 500, Loss: 0.6827\n",
      "Iter: 550, Loss: 0.6811\n",
      "Iter: 600, Loss: 0.6793\n",
      "Iter: 650, Loss: 0.6772\n",
      "Iter: 700, Loss: 0.6749\n",
      "Iter: 750, Loss: 0.6722\n",
      "Iter: 800, Loss: 0.6690\n",
      "Iter: 850, Loss: 0.6654\n",
      "Iter: 900, Loss: 0.6611\n",
      "Iter: 950, Loss: 0.6561\n",
      "Iter: 1000, Loss: 0.6502\n",
      "Iter: 1050, Loss: 0.6432\n",
      "Iter: 1100, Loss: 0.6349\n",
      "Iter: 1150, Loss: 0.6251\n",
      "Iter: 1200, Loss: 0.6134\n",
      "Iter: 1250, Loss: 0.5992\n",
      "Iter: 1300, Loss: 0.5821\n",
      "Iter: 1350, Loss: 0.5612\n",
      "Iter: 1400, Loss: 0.5357\n",
      "Iter: 1450, Loss: 0.5047\n",
      "Iter: 1500, Loss: 0.4677\n",
      "Iter: 1550, Loss: 0.4248\n",
      "Iter: 1600, Loss: 0.3774\n",
      "Iter: 1650, Loss: 0.3281\n",
      "Iter: 1700, Loss: 0.2798\n",
      "Iter: 1750, Loss: 0.2356\n",
      "Iter: 1800, Loss: 0.1971\n",
      "Iter: 1850, Loss: 0.1649\n",
      "Iter: 1900, Loss: 0.1385\n",
      "Iter: 1950, Loss: 0.1173\n",
      "Iter: 2000, Loss: 0.1003\n",
      "Iter: 2050, Loss: 0.0865\n",
      "Iter: 2100, Loss: 0.0754\n",
      "Iter: 2150, Loss: 0.0663\n",
      "Iter: 2200, Loss: 0.0588\n",
      "Iter: 2250, Loss: 0.0526\n",
      "Iter: 2300, Loss: 0.0474\n",
      "Iter: 2350, Loss: 0.0429\n",
      "Iter: 2400, Loss: 0.0392\n",
      "Iter: 2450, Loss: 0.0359\n",
      "Iter: 2500, Loss: 0.0331\n",
      "Iter: 2550, Loss: 0.0306\n",
      "Iter: 2600, Loss: 0.0285\n",
      "Iter: 2650, Loss: 0.0265\n",
      "Iter: 2700, Loss: 0.0248\n",
      "Iter: 2750, Loss: 0.0233\n",
      "Iter: 2800, Loss: 0.0220\n",
      "Iter: 2850, Loss: 0.0207\n",
      "Iter: 2900, Loss: 0.0196\n",
      "Iter: 2950, Loss: 0.0186\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3000\n",
    "\n",
    "for step in range(EPOCHS):    \n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(features, labels, step)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, W4, b1, b2, b3, b4]))\n",
    "        if step % 50 == 0:\n",
    "            loss_value = loss_fn(neural_net(features, step),labels)\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data, step),y_data)\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard 실행\n",
    "- 모델을 만드는 과정에서 도출된 weight나 bias, 학습과정에서 나오는 loss를 시각화 해주는 도구\n",
    "- `pip install tersorboard`를 하고 http://127.0.0.1:6006 에서 확인 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8832), started 0:07:57 ago. (Use '!kill 8832' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4d2d8d081d89d59b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4d2d8d081d89d59b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Start TensorBoard through the command line or within a notebook experience. \n",
    "The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. \n",
    "On the command line, run the same command without \"%\".'''\n",
    "\n",
    "%tensorboard --logdir logs/xor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
