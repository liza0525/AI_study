{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-1 XOR - Logistic Regression - Eager Execution\n",
    "> Logistic Regression으로 XOR 표현하기\n",
    "\n",
    "- eager execution : 즉시 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 Library 선언 및 Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777) # for reproducibility\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 데이터 준비 및 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQRUlEQVR4nO3db4xcV33G8e9Tmwgof0Lxgqgdajcyf9wqQWETohbaAGqx0xcWFaoSUKJGqawUQnmZCAlQ674oqlohRMCyIieCtlgVRMSggFW1glRKU7yWEicmDd06IdnaUTZAKQqttrZ/fTHjMl3P2mtn7gzj8/1Iq517z9mZ39ldnWfOnZl7U1VIktr1c5MuQJI0WQaBJDXOIJCkxhkEktQ4g0CSGrd20gWcq3Xr1tXGjRsnXYYkTZWDBw8+V1Uzw9qmLgg2btzI3NzcpMuQpKmS5HsrtXloSJIaZxBIUuMMAklqnEEgSY0zCCSpcZ0FQZI9SZ5N8ugK7Uny6STzSQ4luaKrWgCOHYNLL4VnnunyUSSpIx1OYl2uCO4Gtp6hfRuwuf+1A/hch7Wwcyc8+WTvuyRNnQ4nsc6CoKruB35whi7bgc9Xz4PAxUle10Utx47BXXfByZO9764KJE2VjiexSb5GsB54emB7ob/vNEl2JJlLMre4uHjOD7RzZ+/3B3DihKsCSVOm40lskkGQIfuGXiWnqnZX1WxVzc7MDP2E9IpOBenSUm97aclVgaQpMoZJbJJBsABcMrC9ATg66gcZDNJTXBVImhpjmMQmGQT7gBv77x66GvhRVR0b+YPs+2mQnrK0BPfeO+pHkqQOjGES6+ykc0m+CFwDrEuyAHwCeBFAVe0C7gOuBeaBnwA3dVHHwkIX9ypJYzKGSayzIKiq68/SXsCHunp8SdLq+MliSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIa12kQJNma5PEk80luH9L+yiRfTfJwksNJbuqyHknS6ToLgiRrgDuAbcAW4PokW5Z1+xDwnaq6HLgG+IskF3VVkyTpdF2uCK4C5qvqSFUtAXuB7cv6FPDyJAFeBvwAON5hTZKkZboMgvXA0wPbC/19gz4DvBk4CjwCfKSqTi6/oyQ7kswlmVtcXOyqXklqUpdBkCH7atn2e4CHgF8E3gJ8JskrTvuhqt1VNVtVszMzM6OuU5Ka1mUQLACXDGxvoPfMf9BNwD3VMw88Abypw5okSct0GQQHgM1JNvVfAL4O2Lesz1PAuwGSvBZ4I3Ckw5okScus7eqOq+p4kluB/cAaYE9VHU5yS799F7ATuDvJI/QOJd1WVc91VZMk6XSdBQFAVd0H3Lds366B20eB3+6yBknSmfnJYklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4ToMgydYkjyeZT3L7Cn2uSfJQksNJvtVlPZKk063t6o6TrAHuAH4LWAAOJNlXVd8Z6HMx8Flga1U9leQ1XdUjSRquyxXBVcB8VR2pqiVgL7B9WZ/3A/dU1VMAVfVsh/VIkoboMgjWA08PbC/09w16A/CqJN9McjDJjcPuKMmOJHNJ5hYXFzsqV5La1GUQZMi+Wra9Fngr8DvAe4CPJXnDaT9UtbuqZqtqdmZmZvSVSlLDOnuNgN4K4JKB7Q3A0SF9nquq54Hnk9wPXA58t8O6JEkDulwRHAA2J9mU5CLgOmDfsj73Au9IsjbJS4G3AY91WJMkaZnOVgRVdTzJrcB+YA2wp6oOJ7ml376rqh5L8g3gEHASuLOqHu2qJknS6VK1/LD9z7bZ2dmam5ubdBmSNFWSHKyq2WFtfrJYkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklq3BmDIMkrklw6ZP9l3ZUkSRqnFYMgye8B/wJ8uX9h+SsHmu/uujBJ0nicaUXwUeCtVfUW4CbgC0l+t9827OpjkqQpdKbrEaypqmMAVfXtJO8EvpZkA6dfclKSNKXOtCL48eDrA/1QuAbYDvxKx3VJksbkTEHwh8DPJdlyakdV/RjYCvxB14VJksZjxSCoqoer6l+Bv01yW3peAvwl8MGxVShJ6tRqPkfwNuAS4AF6F6Q/Cvx6l0VJksZnNUHwP8B/AS8BXgw8UVUnO61KkjQ2qwmCA/SC4Erg7cD1Sb7UaVWSpLE509tHT7m5qub6t58Btie5ocOaJEljdNYVwUAIDO77QjflSJLGzZPOSVLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhrXaRAk2Zrk8STzSW4/Q78rk5xI8r4u65Ekna6zIEiyBrgD2AZsoXeOoi0r9PsksL+rWiRJK+tyRXAVMF9VR6pqCdhL7+pmy30Y+DLwbIe1SJJW0GUQrAeeHthe6O/7P0nWA+8Fdp3pjpLsSDKXZG5xcXHkhUpSy7oMggzZt/yi958CbquqE2e6o6raXVWzVTU7MzMzqvokSazuNNTna4Helc1O2UDv6maDZoG9SQDWAdcmOV5VX+mwLknSgC6D4ACwOckm4N+B64D3D3aoqk2nbie5G/iaISBJ49VZEFTV8SS30ns30BpgT1UdTnJLv/2MrwtIksajyxUBVXUfcN+yfUMDoKp+v8taJEnD+cliSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1LhOgyDJ1iSPJ5lPcvuQ9g8kOdT/eiDJ5V3WI0k6XWdBkGQNcAewDdgCXJ9ky7JuTwC/WVWXATuB3V3VI0karssVwVXAfFUdqaolYC+wfbBDVT1QVT/sbz4IbOiwHknSEF0GwXrg6YHthf6+ldwMfH1YQ5IdSeaSzC0uLo6wRElSl0GQIftqaMfknfSC4LZh7VW1u6pmq2p2ZmZmhCVKktZ2eN8LwCUD2xuAo8s7JbkMuBPYVlXf77AeSdIQXa4IDgCbk2xKchFwHbBvsEOS1wP3ADdU1Xc7rEWStILOVgRVdTzJrcB+YA2wp6oOJ7ml374L+DjwauCzSQCOV9VsVzVJkk6XqqGH7X9mzc7O1tzc3KTLkKSpkuTgSk+0/WSxJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmN6zQIkmxN8niS+SS3D2lPkk/32w8luaKzYo4dg0svhWee6ewhJKkrXU5hnQVBkjXAHcA2YAtwfZIty7ptAzb3v3YAn+uqHnbuhCef7H2XpCnT5RTW5YrgKmC+qo5U1RKwF9i+rM924PPV8yBwcZLXjbySY8fgrrvg5Mned1cFkqZI11NYl0GwHnh6YHuhv+9c+5BkR5K5JHOLi4vnXsnOnb3fIMCJE64KJE2VrqewLoMgQ/bVefShqnZX1WxVzc7MzJxbFaeidGmpt7205KpA0tQYxxTWZRAsAJcMbG8Ajp5HnxdmMEpPcVUgaUqMYwrrMggOAJuTbEpyEXAdsG9Zn33Ajf13D10N/Kiqjo20in37fhqlpywtwb33jvRhJKkL45jC1o7urv6/qjqe5FZgP7AG2FNVh5Pc0m/fBdwHXAvMAz8Bbhp5IQsLI79LSRqXcUxhnQUBQFXdR2+yH9y3a+B2AR/qsgZJ0pn5yWJJapxBIEmNMwgkqXEGgSQ1Lr3Xa6dHkkXge+f54+uA50ZYzjRwzG1wzG14IWP+paoa+oncqQuCFyLJXFXNTrqOcXLMbXDMbehqzB4akqTGGQSS1LjWgmD3pAuYAMfcBsfchk7G3NRrBJKk07W2IpAkLWMQSFLjLsggSLI1yeNJ5pPcPqQ9ST7dbz+U5IpJ1DlKqxjzB/pjPZTkgSSXT6LOUTrbmAf6XZnkRJL3jbO+LqxmzEmuSfJQksNJvjXuGkdtFf/br0zy1SQP98c8+rMYj1GSPUmeTfLoCu2jn7+q6oL6onfK638Dfhm4CHgY2LKsz7XA1+ldIe1q4J8nXfcYxvxrwKv6t7e1MOaBfv9A7yy475t03WP4O18MfAd4fX/7NZOuewxj/ijwyf7tGeAHwEWTrv0FjPk3gCuAR1doH/n8dSGuCK4C5qvqSFUtAXuB7cv6bAc+Xz0PAhcned24Cx2hs465qh6oqh/2Nx+kdzW4abaavzPAh4EvA8+Os7iOrGbM7wfuqaqnAKpq2se9mjEX8PIkAV5GLwiOj7fM0amq++mNYSUjn78uxCBYDzw9sL3Q33eufabJuY7nZnrPKKbZWcecZD3wXmAXF4bV/J3fALwqyTeTHExy49iq68ZqxvwZ4M30LnP7CPCRqlp2cccLysjnr04vTDMhGbJv+XtkV9Nnmqx6PEneSS8I3t5pRd1bzZg/BdxWVSd6Txan3mrGvBZ4K/Bu4CXAPyV5sKq+23VxHVnNmN8DPAS8C7gU+Lsk/1hV/9lxbZMy8vnrQgyCBeCSge0N9J4pnGufabKq8SS5DLgT2FZV3x9TbV1ZzZhngb39EFgHXJvkeFV9ZSwVjt5q/7efq6rngeeT3A9cDkxrEKxmzDcBf1a9A+jzSZ4A3gR8ezwljt3I568L8dDQAWBzkk1JLgKuA/Yt67MPuLH/6vvVwI+q6ti4Cx2hs445yeuBe4AbpvjZ4aCzjrmqNlXVxqraCHwJ+OAUhwCs7n/7XuAdSdYmeSnwNuCxMdc5SqsZ81P0VkAkeS3wRuDIWKscr5HPXxfciqCqjie5FdhP7x0He6rqcJJb+u276L2D5FpgHvgJvWcUU2uVY/448Grgs/1nyMdris/cuMoxX1BWM+aqeizJN4BDwEngzqoa+jbEabDKv/NO4O4kj9A7bHJbVU3t6amTfBG4BliXZAH4BPAi6G7+8hQTktS4C/HQkCTpHBgEktQ4g0CSGmcQSFLjDAJJapxBII1Qkm8k+Y8kX5t0LdJqGQTSaP05cMOki5DOhUEgnYf+NQ4OJXlxkp/vnwf/V6vq74EfT7o+6VxccJ8slsahqg4k2Qf8Kb2Tu/3VNH+CV20zCKTz9yf0zoXz38AfTbgW6bx5aEg6f79A70IoLwdePOFapPNmEEjnbzfwMeCvgU9OuBbpvHloSDoP/St/Ha+qv0myBnggybuAP6Z3LvyX9c8ceXNV7Z9krdLZePZRSWqch4YkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWrc/wJBV9aPpKHk7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "         [0, 1],\n",
    "         [1, 0],\n",
    "         [1, 1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "plt.scatter(x_data[0][0], x_data[0][1], c='red', marker='^')\n",
    "plt.scatter(x_data[3][0], x_data[3][1], c='red', marker='^')\n",
    "plt.scatter(x_data[1][0], x_data[1][1], c='blue', marker='^')\n",
    "plt.scatter(x_data[2][0], x_data[2][1], c='blue', marker='^')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Eager\n",
    "- Tensorlow Data API를 이용해 dataset에 학습 시킬 값을 담는다.\n",
    "- preprocess function으로 Data type을 맞춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- W, b는 0이나 random 값으로 초기화\n",
    "- Sigmoid 함수로 가설 선언\n",
    "- 가설 검증할 Cost Function(또는 Loss Function) 선언\n",
    "- GradientTape으로 경사값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.]\n",
      " [0.]], b = [0.]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros((2,1)), name='weight')\n",
    "b = tf.Variable(tf.zeros((1,)), name='bias')\n",
    "print(\"W = {}, b = {}\".format(W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features), features, labels)\n",
    "        return tape.gradient(loss_value, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습\n",
    "- 학습 시킨 후 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 100, Loss: 0.6931\n",
      "Iter: 200, Loss: 0.6931\n",
      "Iter: 300, Loss: 0.6931\n",
      "Iter: 400, Loss: 0.6931\n",
      "Iter: 500, Loss: 0.6931\n",
      "Iter: 600, Loss: 0.6931\n",
      "Iter: 700, Loss: 0.6931\n",
      "Iter: 800, Loss: 0.6931\n",
      "Iter: 900, Loss: 0.6931\n",
      "Iter: 1000, Loss: 0.6931\n",
      "W = [[0.]\n",
      " [0.]], b= [0.]\n",
      "Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "        if step % 100 == 0:\n",
    "            print('Iter: {}, Loss: {:.4f}'.format(step, loss_fn(logistic_regression(features), features, labels)))\n",
    "print('W = {}, b= {}'.format(W.numpy(), b.numpy()))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(logistic_regression(x_data), y_data)\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위와 같이 Linear Regression만으로는 XOR 데이터를 학습시킬 수 없다.\n",
    "- Test Accuracy를 50%에서 더 올릴 방법이 없기 때문\n",
    "- 따라서, Neural Net을 구성하여 XOR 데이터를 학습하는 모델을 만들어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net 구성\n",
    "- 3 Layer Nerual Network를 구성하여 모델을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([2, 1]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([1,]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1,]), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([2, 1]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([1,]), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "    layer3 = tf.concat([layer1, layer2], -1)\n",
    "    layer3 = tf.reshape(layer3, shape = [-1, 2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad_nn(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features), labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss:0.8487\n",
      "Iter: 5000, Loss:0.6847\n",
      "Iter: 10000, Loss:0.6610\n",
      "Iter: 15000, Loss:0.6154\n",
      "Iter: 20000, Loss:0.5722\n",
      "Iter: 25000, Loss:0.5433\n",
      "Iter: 30000, Loss:0.5211\n",
      "Iter: 35000, Loss:0.4911\n",
      "Iter: 40000, Loss:0.4416\n",
      "Iter: 45000, Loss:0.3313\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50000\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad_nn(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W1, W2, W3, b1, b2, b3]))\n",
    "        if step % 5000 == 0:\n",
    "            print('Iter: {}, Loss:{:.4f}'.format(step, loss_fn(neural_net(features), labels)))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data), y_data)\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class를 이용하여 Neural Net 구성하기\n",
    "- Neural Net 클래스를 만들어 모델링\n",
    "- 위에서 설정한 weights / bias 값과 구현한 function을 하나의 NN class로 만든다.(재사용성을 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset은 3번째 cell에서 선언했으므로 생략\n",
    "nb_classes = 10\n",
    "\n",
    "class wide_deep_nn():\n",
    "    def __init__(self, nb_classes):\n",
    "        super(wide_deep_nn, self).__init__()        \n",
    "     \n",
    "        self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name='weight1')\n",
    "        self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name='bias1')\n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight2')\n",
    "        self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name='bias2')\n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight3')\n",
    "        self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name='bias3')\n",
    "\n",
    "        self.W4 = tf.Variable(tf.random.normal((nb_classes, 1)), name='weight4')\n",
    "        self.b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
    "        \n",
    "        self.variables = [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4]\n",
    "        \n",
    "    def preprocess_data(self, features, labels):\n",
    "        features = tf.cast(features, tf.float32)\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        return features, labels\n",
    "        \n",
    "    def deep_nn(self, features):\n",
    "        layer1 = tf.sigmoid(tf.matmul(features, self.W1) + self.b1)\n",
    "        layer2 = tf.sigmoid(tf.matmul(layer1, self.W2) + self.b2)\n",
    "        layer3 = tf.sigmoid(tf.matmul(layer2, self.W3) + self.b3)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4) + self.b4)\n",
    "        return hypothesis\n",
    "\n",
    "    def loss_fn(self, hypothesis, features, labels):\n",
    "        cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def accuracy_fn(self, hypothesis, labels):\n",
    "        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def grad(self, features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.loss_fn(self.deep_nn(features),features,labels)\n",
    "        return tape.gradient(loss_value,self.variables)\n",
    "\n",
    "    def fit(self, dataset, EPOCHS=20000, verbose=500):\n",
    "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "        for step in range(EPOCHS):\n",
    "            for features, labels  in dataset:\n",
    "                features, labels = self.preprocess_data(features, labels)\n",
    "                grads = self.grad(features, labels)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))\n",
    "                if step % verbose == 0:\n",
    "                    print(\"Iter: {}, Loss: {:.4f}\".format(step, self.loss_fn(self.deep_nn(features),features,labels)))\n",
    "\n",
    "    def test_model(self,x_data, y_data):\n",
    "        x_data, y_data = self.preprocess_data(x_data, y_data)\n",
    "        test_acc = self.accuracy_fn(self.deep_nn(x_data),y_data)\n",
    "        print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 1.2497\n",
      "Iter: 500, Loss: 0.6848\n",
      "Iter: 1000, Loss: 0.6823\n",
      "Iter: 1500, Loss: 0.6794\n",
      "Iter: 2000, Loss: 0.6761\n",
      "Iter: 2500, Loss: 0.6722\n",
      "Iter: 3000, Loss: 0.6676\n",
      "Iter: 3500, Loss: 0.6622\n",
      "Iter: 4000, Loss: 0.6558\n",
      "Iter: 4500, Loss: 0.6483\n",
      "Iter: 5000, Loss: 0.6398\n",
      "Iter: 5500, Loss: 0.6300\n",
      "Iter: 6000, Loss: 0.6188\n",
      "Iter: 6500, Loss: 0.6055\n",
      "Iter: 7000, Loss: 0.5898\n",
      "Iter: 7500, Loss: 0.5708\n",
      "Iter: 8000, Loss: 0.5477\n",
      "Iter: 8500, Loss: 0.5198\n",
      "Iter: 9000, Loss: 0.4863\n",
      "Iter: 9500, Loss: 0.4469\n",
      "Iter: 10000, Loss: 0.4024\n",
      "Iter: 10500, Loss: 0.3544\n",
      "Iter: 11000, Loss: 0.3057\n",
      "Iter: 11500, Loss: 0.2592\n",
      "Iter: 12000, Loss: 0.2175\n",
      "Iter: 12500, Loss: 0.1817\n",
      "Iter: 13000, Loss: 0.1521\n",
      "Iter: 13500, Loss: 0.1280\n",
      "Iter: 14000, Loss: 0.1086\n",
      "Iter: 14500, Loss: 0.0931\n",
      "Iter: 15000, Loss: 0.0806\n",
      "Iter: 15500, Loss: 0.0705\n",
      "Iter: 16000, Loss: 0.0621\n",
      "Iter: 16500, Loss: 0.0553\n",
      "Iter: 17000, Loss: 0.0496\n",
      "Iter: 17500, Loss: 0.0447\n",
      "Iter: 18000, Loss: 0.0406\n"
     ]
    }
   ],
   "source": [
    "model = wide_deep_nn(nb_classes)\n",
    "model.fit(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
